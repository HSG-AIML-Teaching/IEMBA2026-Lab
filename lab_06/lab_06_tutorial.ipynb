{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNMrbY1MUml"
      },
      "source": [
        "# **Using OpenAI API with Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Securely Handling Sensitive Data in Colab**"
      ],
      "metadata": {
        "id": "qgLQUPnAXPDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use OpenAI's GPT models the API key needs to be set up.\n",
        "\n",
        "Before being able to use the Responses API\n",
        "endpoint we need to authenticate ourselves using the **API key**. Having finished the preparatory assignments you should have your own API key which you can use during the course as well as the post assignment.\n",
        "\n",
        "When working with sensitive information like API keys or passwords in Google Colab, it's crucial to handle data securely. Two common approaches for this are using **Colab's Secrets Manager**, which stores and retrieves secrets without exposing them in the notebook, and `getpass`, a Python function that securely prompts users to input secrets during runtime without showing them. Both methods help ensure your sensitive data remains protected."
      ],
      "metadata": {
        "id": "U7B32sA1XDs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Option 1: Using Google Colab Secrets Manager**"
      ],
      "metadata": {
        "id": "aiuHoVgAW58K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vILGOv5Dau3Q"
      },
      "source": [
        "Google Colab provides an integrated Secrets Manager, allowing you to securely store and retrieve sensitive information such as API keys or authentication tokens without hardcoding them in your notebook.\n",
        "\n",
        "**Step 1: Store Your Secret in Colab**\n",
        "\n",
        "1.   In the Colab notebook, navigate to the left sidebar.\n",
        "2.   Click on the **“Secrets”** tab (represented by a key icon).\n",
        "3. Add your secret by clicking on **“+ Add a new secret”**. For example, you might add a secret called `OPENAI_API_KEY` with the value of your API key.\n",
        "\n",
        "**Step 2: Access the Secret in Your Notebook**\n",
        "\n",
        "Once you've added a secret, you can easily access it from within the notebook.\n",
        "\n",
        "`OPENAI_API_KEY` is the name of the secret you've added in the Colab Secrets Manager. It will be retrieved securely without having to expose the key in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D6e8iVJ-L9r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Option 2: Using Python's `getpass` for Secret Input**"
      ],
      "metadata": {
        "id": "ApcrD_OGX7Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, the `getpass` module allows you to securely input secrets (e.g., passwords or API keys) during runtime, making sure they're not visible in the notebook output.\n",
        "\n",
        "Here, the `getpass.getpass` function prompts the user to enter the secret without displaying it as they type, ensuring that sensitive data isn't exposed."
      ],
      "metadata": {
        "id": "mVUeKxs_WzXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
      ],
      "metadata": {
        "id": "M_fgOXZ7W0xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OdoJQyhhjPv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing OpenAI**"
      ],
      "metadata": {
        "id": "5iqfA8ZvYJBN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50s3eLZ7xUnl"
      },
      "source": [
        "In this notebook you will learn how to use OpenAI API with Python for various tasks such as **text generation**, **speech-to-text**,  **text-to-speech**, and  **image generation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUayPEw170rz"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SSfF_F8ADV"
      },
      "source": [
        "## **Text Generation**\n",
        "\n",
        "OpenAI's text generation models (often called large language models, or LLMs) generate text in response to input instructions. These models can understand and generate natural language and code, and can be guided using carefully designed prompts.\n",
        "\n",
        "Text generation is accessed through the **Responses API**, which is the unified API for generating text and other outputs from OpenAI models.\n",
        "\n",
        "For an overview of available models, see:  \n",
        "https://platform.openai.com/docs/models\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prompts and Inputs**\n",
        "\n",
        "Models generate text in response to **inputs**, often referred to as *prompts*. Designing a prompt is essentially how you “program” a language model.\n",
        "\n",
        "A prompt may include:\n",
        "\n",
        "- clear instructions describing the task  \n",
        "- background context about the topic or audience  \n",
        "- constraints or formatting requirements  \n",
        "- examples of desired behavior  \n",
        "\n",
        "Well-designed prompts significantly improve the quality and reliability of the generated output.\n"
      ],
      "metadata": {
        "id": "CIs2x69bmpgu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SartI438kJ7Y"
      },
      "source": [
        "\n",
        "### **Using the Responses API**\n",
        "\n",
        "To generate text, you send an input to a model and receive the model's generated output in response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCNMr6_cm4YC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09QaI2PqzQbs"
      },
      "source": [
        "#### Example: Simple instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDQG-s4EzZmM"
      },
      "outputs": [],
      "source": [
        "#Example with a simple instruction\n",
        "\n",
        "#Example models to try\n",
        "#\n",
        "#Frontier models:\n",
        "#gpt-5.2\n",
        "#gpt-5-mini\n",
        "#gpt-5-nano\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    input=\"Explain generative AI to a 6 year old.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated text can be accessed using `response.output_text`.\n"
      ],
      "metadata": {
        "id": "wiVpCBpZrZvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "bOY1opQwSJeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optional parameters\n",
        "\n",
        "Some models support optional parameters that influence how text is generated.\n",
        "\n",
        "- **`temperature`**  \n",
        "  Controls the randomness of generated text.\n",
        "  \n",
        "  - Lower values → more focused and deterministic responses  \n",
        "  - Higher values → more creative and varied responses  \n",
        "\n",
        "> **Note:** For some reasoning-focused models, the `temperature` parameter may have limited or no effect.\n"
      ],
      "metadata": {
        "id": "pQNHEWIYneRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structured input (optional)\n",
        "\n",
        "For more complex prompts, you can provide structured input using message-like objects. This can help organize instructions, but is not required.\n",
        "\n",
        "Each message may include:\n",
        "- **`role`** (for example, `system` or `user`)\n",
        "- **`content`** (the text of the message)\n",
        "\n",
        "Roles are optional and are mainly used for clarity in longer or multi-step prompts.\n"
      ],
      "metadata": {
        "id": "Dv107H4MoQgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"You explain concepts clearly and simply.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain generative AI in simple terms.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "8hHgTm3roUPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can make OpenAI output (such as ChatGPT responses) more readable in a Jupyter Notebook by displaying it as formatted Markdown using IPython's `display` functions.\n"
      ],
      "metadata": {
        "id": "Dhitn7qGHeQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "rp_vQ8s_FkAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.output_text))"
      ],
      "metadata": {
        "id": "qDLsCcaTHj-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9p6YUWm0v_Z"
      },
      "source": [
        "### **Prompting Guidelines**\n",
        "\n",
        "The following guidelines help you write effective prompts for language models.  \n",
        "They apply regardless of whether you use plain text input or structured messages with roles.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Be clear and specific (most important)\n",
        "\n",
        "Avoid vague instructions. Clearly state what you want the model to do, including any constraints or edge cases.\n",
        "\n",
        "**Example:**  \n",
        "“Write a Python function that calculates the Fibonacci sequence. Handle negative inputs and include explanatory comments.”\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "v38464TDoELA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Define the desired output format\n",
        "\n",
        "Tell the model how the response should be structured. This improves readability and makes outputs easier to reuse.\n",
        "\n",
        "**Example:**  \n",
        "“Structure the response with a short introduction, three bullet points, and a one-sentence conclusion.”\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qBQkyc5ZoIOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Limit the scope of the task\n",
        "\n",
        "Narrow prompts lead to more relevant and focused answers.\n",
        "\n",
        "**Example:**  \n",
        "“Recommend three family-friendly activities in Paris. Include a short description for each.”\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bW0FxbbfoL49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Provide relevant context\n",
        "\n",
        "Give background information about the audience, domain, or assumptions when needed.\n",
        "\n",
        "**Example:**  \n",
        "“Explain this concept for an audience with no prior programming experience.”\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "srILCBMhoO7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Specify tone and style\n",
        "\n",
        "Explicitly describe the desired tone, level of formality, or writing style.\n",
        "\n",
        "**Example:**  \n",
        "“Use a friendly, conversational tone and simple language.”\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vYWwGUZpoSgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Use examples when helpful\n",
        "\n",
        "Providing examples of desired input–output behavior can improve results, especially for classification or formatting tasks.\n",
        "\n",
        "(This technique is known as *few-shot prompting* and is covered in the next section.)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gP2TAJvToamJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Key takeaway**\n",
        "\n",
        "Clear instructions matter more than prompt structure.  \n",
        "Roles and message formats can help organize prompts, but they are optional."
      ],
      "metadata": {
        "id": "IWTAYsMsoc5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This example demonstrates the most important concept:\n",
        "# clear and specific instructions lead to better outputs.\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Explain generative AI to a 6-year-old using a fun analogy. \"\n",
        "        \"Avoid technical terms and keep the explanation under 150 words.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# The generated text can be accessed directly via response.output_text\n",
        "display(Markdown(response.output_text))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tTBwE3pFkV9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This example shows how output structure and tone\n",
        "# can be controlled directly through instructions.\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Write a short, lighthearted article explaining why hiking is great for beginners.\\n\\n\"\n",
        "        \"Structure the response as:\\n\"\n",
        "        \"- A 2-sentence introduction\\n\"\n",
        "        \"- Three bullet points listing benefits\\n\"\n",
        "        \"- A one-sentence conclusion\\n\\n\"\n",
        "        \"Use simple language and friendly humor.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.output_text))\n"
      ],
      "metadata": {
        "id": "RybeCz-dk-ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This example demonstrates how to tailor responses\n",
        "# to a specific audience using instructions alone.\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Explain the difference between ML APIs (e.g., Google Cloud Vision, ChatGPT) \"\n",
        "        \"and open-source, off-the-shelf pre-trained models.\\n\\n\"\n",
        "        \"Target audience: executive MBA students.\\n\"\n",
        "        \"Use business-relevant examples and keep the explanation concise.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "display(Markdown(response.output_text))\n"
      ],
      "metadata": {
        "id": "u0YbEfoIlPvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Roles are used here to separate background instructions\n",
        "# from the specific task. This is optional and mainly\n",
        "# improves readability.\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"Explain complex topics in simple, child-friendly language. \"\n",
        "                \"Use fun analogies and avoid technical jargon.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain generative AI to a 6-year-old.\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(Markdown(response.output_text))\n"
      ],
      "metadata": {
        "id": "SnQqCHJ-ldIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nWaZASY05os"
      },
      "source": [
        "#### **Few-shot Prompting**\n",
        "\n",
        "Few-shot prompting is a technique where you include a small number of examples (typically 2-5) directly in the prompt to demonstrate the desired format, style, or behavior for a task.\n",
        "\n",
        "Each example consists of an input and the corresponding expected output. By observing these input-output pairs, the model can infer the underlying pattern and apply it to new, unseen inputs—without explicitly programming the rules.\n",
        "\n",
        "Few-shot prompting is especially useful for tasks such as classification, translation, rewriting, and style transformation, where showing examples is often more effective than describing the logic in words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following examples, we use few-shot prompting to demonstrate how examples can guide the model's behavior.\n"
      ],
      "metadata": {
        "id": "jc0ofweCksI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SGDF_Uaz4k2"
      },
      "outputs": [],
      "source": [
        "# Few-shot prompting example:\n",
        "# The prompt includes a small number of example input–output pairs\n",
        "# to demonstrate how corporate jargon should be translated into\n",
        "# simple, plain English. These examples act as instructions that\n",
        "# guide the model’s behavior.\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Translate corporate jargon into plain English.\"\n",
        "        \"Examples:\"\n",
        "        \"Corporate: New synergies will help drive top-line growth.\"\n",
        "        \"Plain English: Things working well together will increase revenue.\"\n",
        "        \"Corporate: Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"\n",
        "        \"Plain English: Let's talk later when we're less busy about how to do better.\\n\\n\"\n",
        "        \"Corporate: This late pivot means we don't have time to boil the ocean for the client deliverable.\"\n",
        "        \"Plain English:\"\n",
        "    ),\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(response.output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COycJ3lhbqt3"
      },
      "outputs": [],
      "source": [
        "# Few-shot classification example:\n",
        "# Example title–category pairs are provided inline to show\n",
        "# how sports-related titles should be classified.\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Classify sports-related titles into categories. \"\n",
        "        \"Examples: \"\n",
        "        \"Title: Paris auf der Saslong nicht zu biegen. Category: Ski Alpin. \"\n",
        "        \"Title: Kobayashi geht in Führung. Category: Skispringen. \"\n",
        "        \"Title: Hütter fährt in Abfahrt aufs Podest. Category: Ski Alpin. \"\n",
        "        \"Title: Schweizer Freudentag: Flury brilliert vor Hählen. Category: Ski Alpin. \"\n",
        "        \"Title: Arsenal unterliegt Tottenham im Derby. Category: Fussball. \"\n",
        "        \"Title: Seoanes Gladbach muss sich mit Remis begnügen. Category:\"\n",
        "    ),\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPh9-whtnxMM"
      },
      "source": [
        "### **Customizing the Responses API for Targeted Tasks**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll customize model behavior for targeted tasks by creating **small helper functions**. Each helper function uses:\n",
        "\n",
        "- **static instructions** (the reusable part of the prompt), and  \n",
        "- **dynamic input** (the user-provided text passed as a parameter).\n",
        "\n",
        "This pattern represents a foundational way to customize model behavior.  Higher-level tools such as prompt templates and agent frameworks build on the same idea by abstracting static instructions and dynamic inputs.\n",
        "\n",
        "> In Lab07, we will build on this concept using LangChain Prompt Templates to define reusable prompts with placeholders for dynamic input.\n"
      ],
      "metadata": {
        "id": "hDLt4WS_602Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRIFkT3Uww7Y"
      },
      "source": [
        "#### Example: Text Translation (English → German)\n",
        "\n",
        "The function below keeps the translation instructions fixed while passing the text to translate dynamically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSiqZpvn0A4"
      },
      "outputs": [],
      "source": [
        "# The prompt is built as a formatted string:\n",
        "# static instructions define the task, and the variable `text`\n",
        "# is inserted dynamically using a Python f-string.\n",
        "\n",
        "# Example:\n",
        "# If text = \"My name is Barbara.\",\n",
        "# the line\n",
        "# f\"English: {text}\" becomes \"English: My name is Barbara.\"\n",
        "\n",
        "\n",
        "def translateFromEnglishToGerman(text):\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        input=(\n",
        "          \"Translate the following sentence from English to German. \"\n",
        "          \"Return only the German translation.\\n\\n\"\n",
        "          f\"English: {text}\\n\"\n",
        "          \"German:\"\n",
        "        ),\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return response.output_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4-PKRbBoNgJ"
      },
      "outputs": [],
      "source": [
        "translatedText = translateFromEnglishToGerman(\"My name is Barbara. What is yours?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSXYuZZUo_SC"
      },
      "outputs": [],
      "source": [
        "print(translatedText)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qDuCeFnGsz"
      },
      "source": [
        "#### Example: Image Prompt Generation\n",
        "\n",
        "The function below turns a short idea into a richer image-generation prompt. The instructions are fixed, and the user’s idea is passed in dynamically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AX8KmJOmCq5"
      },
      "outputs": [],
      "source": [
        "def createImagePrompt(text):\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        input=(\n",
        "            \"You are a skilled prompt generator. Rewrite the user's idea into a vivid, \"\n",
        "            \"high-quality image-generation prompt. Include subject, environment, lighting, \"\n",
        "            \"mood, composition, and style. Keep it to 1–2 sentences.\\n\\n\"\n",
        "            f\"Idea: {text}\\n\"\n",
        "            \"Image prompt:\"\n",
        "        )\n",
        "    )\n",
        "    return response.output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3XfFfSCrNwA"
      },
      "outputs": [],
      "source": [
        "imagePrompt = createImagePrompt(\"nice and cinematic mountain ranges\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(imagePrompt))"
      ],
      "metadata": {
        "id": "i7rG6uNi92zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWOx8VRM3BBn"
      },
      "source": [
        "## **Text to Speech Generation**\n",
        "\n",
        "OpenAI provides text-to-speech (TTS) capabilities that allow you to convert generated text into spoken audio. This is useful for applications such as:\n",
        "\n",
        "- narrating written content\n",
        "- producing spoken audio in multiple languages\n",
        "- generating audio files programmatically\n",
        "\n",
        "Text-to-speech models support multiple built-in voices, such as  \n",
        "`alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, and `shimmer`.\n",
        "\n",
        "For an overview of supported languages and voices, see:  \n",
        "https://platform.openai.com/docs/guides/text-to-speech\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have generated and transformed text. We can now use this text as input to generate spoken audio.\n"
      ],
      "metadata": {
        "id": "8gdquP2XuFhQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGHU8qvFSgM"
      },
      "source": [
        "### **Audio Generation Example**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function converts text into spoken audio and streams\n",
        "# the generated speech directly into an MP3 file.\n",
        "\n",
        "def generateTextToSpeech(text):\n",
        "    speech_file_path = \"speech.mp3\"\n",
        "    with client.audio.speech.with_streaming_response.create(\n",
        "        model=\"gpt-4o-mini-tts\",\n",
        "        voice=\"coral\",\n",
        "        input=text\n",
        "    ) as response:\n",
        "        response.stream_to_file(speech_file_path)\n",
        "        return speech_file_path\n"
      ],
      "metadata": {
        "id": "TaNh8ma7ncXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EILjTImd3VQa"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "fileName=generateTextToSpeech(\"Today is a wonderful day to build something people love!\")\n",
        "ipd.Audio(filename=fileName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymJNZuYwsK6Y"
      },
      "source": [
        "The following example combines text translation and text-to-speech into a simple end-to-end pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_0Z7SvqoZ7W"
      },
      "outputs": [],
      "source": [
        "# Simple pipeline: text translation (reusing the functions we created earlier) followed by speech generation\n",
        "\n",
        "translatedText = translateFromEnglishToGerman(\n",
        "    \"My name is Barbara. What is yours?\"\n",
        ")\n",
        "\n",
        "fileName = generateTextToSpeech(translatedText)\n",
        "ipd.Audio(filename=fileName)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TBpd5iXBtWL"
      },
      "source": [
        "## **Speech to Text (Audio API)**\n",
        "\n",
        "OpenAI provides speech-to-text capabilities that allow you to transcribe spoken audio into text or translate it into English. These capabilities are based on the Whisper model.\n",
        "\n",
        "Speech-to-text can be used to:\n",
        "- transcribe audio in its original language\n",
        "- translate and transcribe audio into English\n",
        "\n",
        "Supported audio formats include `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, and `webm`.  \n",
        "Uploaded files are currently limited to 25 MB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DlsqqTCdDt"
      },
      "source": [
        "### **Transcription Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oml-3lwNt9-3"
      },
      "outputs": [],
      "source": [
        "def transcribeAudio(fileName):\n",
        "  with open(fileName, \"rb\") as audio_file:\n",
        "    transcript = client.audio.transcriptions.create(\n",
        "      model=\"whisper-1\",\n",
        "      file=audio_file,\n",
        "      response_format=\"text\"\n",
        "  )\n",
        "  return transcript"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now download and transcribe a short news podcast.\n"
      ],
      "metadata": {
        "id": "6FPkI4thknpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Podcast.mp3 https://download-media.srf.ch/world/audio/4x4_Podcast_radio/2025/01/4x4_Podcast_radio_AUDI20250128_NR_0038_3c81668cc4664fff9a26020d4eb47f0a.mp3\n"
      ],
      "metadata": {
        "id": "1NP0UZE4MbMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTBllScAvvHZ"
      },
      "outputs": [],
      "source": [
        "# The file name which is passed as a parameter to the function transcribeAudio needs to correspond to the file you have uploaded\n",
        "transcribedAudio= transcribeAudio(\"Podcast.mp3\")\n",
        "display(Markdown(transcribedAudio))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summarizing the transcript**\n",
        "\n",
        " We can now use the transcript as input to generate summaries using a text generation model.\n",
        "\n"
      ],
      "metadata": {
        "id": "E3lM5-IwqS9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Create a bullet list of the different news topics covered in the following \"\n",
        "        \"podcast transcript. Write in German and use 1–2 sentences per bullet point.\\n\\n\"\n",
        "        f\"Transcript:\\n{transcribedAudio}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "summary = response.output_text\n",
        "display(Markdown(summary))\n"
      ],
      "metadata": {
        "id": "wsTyacFVp5Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Focused summarization (topic filtering)**\n",
        "\n",
        "The next example creates a focused summary, restricted to specific topics.\n"
      ],
      "metadata": {
        "id": "86BdzYjDmsdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=(\n",
        "        \"Summarize the following podcast transcript in German, focusing only on topics \"\n",
        "        \"related to artificial intelligence (KI). Limit the summary to a maximum of \"\n",
        "        \"20 sentences.\\n\\n\"\n",
        "        f\"Transcript:\\n{transcribedAudio}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "summary = response.output_text\n",
        "display(Markdown(summary))\n"
      ],
      "metadata": {
        "id": "3D0n_O4PkS_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **From text back to speech**\n",
        "\n",
        "We can reuse the summary as input for text-to-speech generation.\n"
      ],
      "metadata": {
        "id": "pe7EtAOnnbfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileName=generateTextToSpeech(summary)\n",
        "ipd.Audio(filename=fileName)"
      ],
      "metadata": {
        "id": "VDZ10f2unah0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transcription with translation**"
      ],
      "metadata": {
        "id": "BXxrjCqMwIrN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQxMTgU0C9k8"
      },
      "source": [
        "If the original language is not suitable, audio can be transcribed and translated into English.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U66NNQ1vDLit"
      },
      "outputs": [],
      "source": [
        "def transcribeAndTranslateAudio(fileName):\n",
        "    with open(fileName, \"rb\") as audio_file:\n",
        "        transcript = client.audio.translations.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "            response_format=\"text\"\n",
        "        )\n",
        "    return transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjFKaXsZC8xt"
      },
      "outputs": [],
      "source": [
        "display(Markdown(transcribeAndTranslateAudio(\"Podcast.mp3\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy8C5DdGeqxa"
      },
      "source": [
        "### **Working with longer audio files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEvphh0dEAlP"
      },
      "source": [
        "Whisper supports audio files up to 25 MB. Longer audio files must be split into smaller segments before transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0Ef_Lgigs3_"
      },
      "outputs": [],
      "source": [
        "!pip install -q pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbJu3sVD_Ox"
      },
      "outputs": [],
      "source": [
        "# code adapted from: https://platform.openai.com/docs/guides/speech-to-text/longer-inputs\n",
        "\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def segmentAudio(audioFile):\n",
        "  audio = AudioSegment.from_mp3(audioFile)\n",
        "  # PyDub handles time in milliseconds\n",
        "  two_minutes = 2 * 60 * 1000\n",
        "  segment = audio[:two_minutes]\n",
        "  output_file = \"TwoMinutes_\" + audioFile\n",
        "  segment.export(output_file, format=\"mp3\")\n",
        "  return output_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be asked to upload a file to be transcribed.\n",
        "\n",
        "To explore the segmentation capabilities of the Audio API you can experiment with one longer audio files such as https://www.srf.ch/audio/tagesgespraech."
      ],
      "metadata": {
        "id": "t6-3l6s-nNnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Tagesgespraech.mp3 \"https://download-media.srf.ch/world/audio/Tagesgespraech_radio/2025/01/Tagesgespraech_radio_AUDI20250118_NR_0030_bf244ffb90af4470abd2cebd91107e91.mp3?d=ap&assetId=641aa6a1-597f-3c50-bce5-a8e0193e76cc\""
      ],
      "metadata": {
        "id": "TLzgTFedv0aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MinVdrf_jCH8"
      },
      "source": [
        "We can then play the split audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfvSZQwPf1J7"
      },
      "outputs": [],
      "source": [
        "twoMinuteFile = segmentAudio(\"Tagesgespraech.mp3\")\n",
        "ipd.Audio(filename=twoMinuteFile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-yXHgcDjIgu"
      },
      "source": [
        ".. and even translate it to another language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD9freWohDaY"
      },
      "outputs": [],
      "source": [
        "display(Markdown(transcribeAndTranslateAudio(twoMinuteFile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtjqrJXBjza8"
      },
      "source": [
        "To experiment a bit more with Whisper visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb). Moreover, if you want to learn more on prompting with Whisper visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUISYB-lmySg"
      },
      "source": [
        "## **Image Generation**\n",
        "\n",
        "OpenAI models can generate images directly from text prompts. Image generation allows you to create visual content based on natural language descriptions, making it useful for tasks such as illustration, design ideation, and creative exploration.\n",
        "\n",
        "Image generation is driven by a text prompt that describes the desired scene, style, and visual details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating Images from Text**\n",
        "\n",
        "OpenAI provides image-generation models that create images from text prompts. Depending on your account configuration, different image-generation models may be available.\n",
        "\n",
        "To keep this notebook accessible to everyone, we present **two options**:\n",
        "- an older DALL·E-based model that may work without additional account verification\n",
        "- a newer image-generation model that requires account verification\n",
        "\n"
      ],
      "metadata": {
        "id": "m_mm7yNZyEG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 1: DALL·E Image Generation (Legacy Model)\n",
        "\n",
        "This option uses an older DALL·E-based image model.  \n",
        "It may be available without completing additional account verification steps.\n"
      ],
      "metadata": {
        "id": "Y_3HkNXZ2zDC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvUFhEuZm4qS"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=\"a downhill skier in the swiss alps\",\n",
        "  size=\"1024x1024\"\n",
        ")\n",
        "\n",
        "image_url = response.data[0].url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R39ftr4xnXfp"
      },
      "outputs": [],
      "source": [
        "# Load and display the generated image\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "with urllib.request.urlopen(image_url) as url:\n",
        "    img=Image.open(url)\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 2: Image Generation with gpt-image-1 (Account Verification Required)\n",
        "\n",
        "This option uses a newer image-generation model. Access to this model requires account verification.\n"
      ],
      "metadata": {
        "id": "gR9Gzs-02_BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.images.generate(\n",
        "    model=\"gpt-image-1\",\n",
        "    prompt=\"a downhill skier in the Swiss Alps, cinematic lighting\",\n",
        "    size=\"1024x1024\",\n",
        ")\n",
        "\n",
        "b64 = response.data[0].b64_json"
      ],
      "metadata": {
        "id": "hJhrEUKuzyVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "img = Image.open(BytesIO(base64.b64decode(b64)))\n",
        "display(img)"
      ],
      "metadata": {
        "id": "zIQVqWVn41To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Key takeaway**\n",
        "\n",
        "Both options generate images from text prompts using the same workflow.\n",
        "The difference lies in the image-generation model that is available for your account.\n",
        "If the newer model is not accessible, the legacy DALL·E option can still be used.\n"
      ],
      "metadata": {
        "id": "Ps7VGytK3VPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multimodal Example: Analyze the Generated Image**\n",
        "\n",
        "Now that we have generated an image, we can pass it back to a multimodal model together with a text instruction. This allows the model to describe what it sees or extract structured information from the image.\n"
      ],
      "metadata": {
        "id": "bLB2cK6Vs5k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyzeGeneratedImage(image_url):\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        input=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"input_text\", \"text\": \"Describe what you see in this image in 3 bullet points.\"},\n",
        "                {\"type\": \"input_image\", \"image_url\": image_url},\n",
        "            ],\n",
        "        }],\n",
        "    )\n",
        "    return response.output_text"
      ],
      "metadata": {
        "id": "rPUbRg0htIyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we reuse the image_url from the example above\n",
        "\n",
        "analysis = analyzeGeneratedImage(image_url)\n",
        "display(Markdown(analysis))\n"
      ],
      "metadata": {
        "id": "gJ573Bu3tV2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same image can be reused with a different instruction to guide the model toward a new task.\n"
      ],
      "metadata": {
        "id": "Wqcu-YPPwywr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = analyzeGeneratedImage(image_url)\n",
        "display(Markdown(analysis))\n",
        "\n",
        "# Same image, different instruction\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"input_text\", \"text\": \"What might be happening just before this scene?\"},\n",
        "            {\"type\": \"input_image\", \"image_url\": image_url},\n",
        "        ],\n",
        "    }],\n",
        ")\n",
        "\n",
        "display(Markdown(response.output_text))\n"
      ],
      "metadata": {
        "id": "x9gPZ_vEwzBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a multimodal prompt, different input types (such as text and images) are combined in a single message, allowing the model to reason over them together.\n"
      ],
      "metadata": {
        "id": "ofGlqNWPwCEI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ApcrD_OGX7Mb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}